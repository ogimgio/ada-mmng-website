{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680173ad-4569-41d0-aead-d8086d01ab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giomonopoli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/giomonopoli/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/giomonopoli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "import bz2\n",
    "# import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pickle\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efc39825-a272-43a6-b287-f9e4b14d0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data.nosync/processed/quotes-2020-party_labeled_cleaned.json.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d2c4257-936f-4986-a82f-7d6bd32ed5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path_to_file, lines=True, compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dfa530d-4400-48db-8199-9cbd7b6ad895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "589a1edc-8679-4ace-9384-379235d997d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_lb(x):\n",
    "    if x == 'R':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df[\"party_label\"] = df[\"party_label\"].apply(lambda x: change_lb(x))\n",
    "y = df[\"party_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9a6e21-da4c-446e-9e09-bab935653d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_quotation'] = df['quotation'].apply(lambda x: preprocess(x))\n",
    "df = df.drop_duplicates(subset=['clean_quotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d7033a-9590-4db6-bfda-436ac06dfde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct proportion (doesnt change nothing)\n",
    "rows = np.random.choice(df[df[\"party_label\"] == 0].index.values, 100000)\n",
    "sampled_df = df.loc[rows]\n",
    "rows = np.random.choice(df[df[\"party_label\"] == 1].index.values, 100000)\n",
    "sampled2_df = df.loc[rows]\n",
    "balanced_df = pd.concat([sampled_df,sampled2_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c8c400-3d9d-49ef-9fd5-91fb868b8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=195000\n",
    "X_mini = balanced_df['clean_quotation'][:train_size]\n",
    "X_test = balanced_df['clean_quotation'][train_size:train_size + 5000]\n",
    "y_mini = y[:train_size]\n",
    "y_test = y[train_size:train_size + 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad44f4f-d45a-419e-b93e-8355dc0a40db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/2ds3y3yd6319x1125r61pg1c0000gn/T/ipykernel_19078/1011475071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopwords.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimple_tokeniser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords.txt'"
     ]
    }
   ],
   "source": [
    "def simple_tokeniser(text):\n",
    "    return text.split()\n",
    "\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    stop_words = list(map(lambda x: x[:-1], f.readlines()))\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words,tokenizer=simple_tokeniser, min_df=2)\n",
    "train_vectors =vectorizer.fit_transform(X_mini)\n",
    "test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6a09d5b-3b7f-4b43-b464-ea2b6ea23833",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/2ds3y3yd6319x1125r61pg1c0000gn/T/ipykernel_17736/3195599578.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnb_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnb_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#Predict y value for test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(train_vectors, y_mini)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(test_vectors)\n",
    "y_prob = nb_tfidf.predict_proba(test_vectors)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bade6ae5-fcd4-4ca8-bd12-63e78c834c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#il numero di republicani vs democratici non dev esser proprozionale? need to balance out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e42ecc-4d7d-4b11-9779-3b4015842ee8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d3fb4-51ad-42d7-a11f-fda3fdb0056f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "769ef86c-27a4-4ba4-b1f9-a99ff0926f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "tf_vectorizer = CountVectorizer(max_df=0.8, min_df=50,\n",
    "    ngram_range = (1,2),\n",
    "    binary=True,\n",
    "    stop_words=nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "85b6b2d6-fbad-4846-81bf-05e777531a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequencies = tf_vectorizer.fit_transform(df['quotation_cleanD'].values)\n",
    "phrases_df = pd.DataFrame(data=tf_vectorizer.get_feature_names_out(),columns=['phrase'])\n",
    "phrases_df['total_occurrences']=np.array(term_frequencies.sum(axis=0)).reshape(term_frequencies.sum(axis=0).shape[1])\n",
    "phrases_df.sort_values(by=['total_occurrences','phrase'],ascending=False).head(300).to_csv('top_20_overall.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "bcd0761e-3555-4573-9325-3266da7e6ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.datasets import fetch_20newsgroups\\nnewsgroups_train = fetch_20newsgroups(categories=['talk.politics.guns',\\n 'talk.politics.mideast',\\n 'talk.politics.misc',])\""
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(categories=['talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "224e71ad-8ef9-4f3f-bd69-7d3e1b0c3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = fixpath(QUOTES_ALL_TIME_PROCESSED)\n",
    "\n",
    "df = pd.read_json(path, orient='records', lines=True)\n",
    "def downsample(df:pd.DataFrame, label_col_name:str) -> pd.DataFrame:\n",
    "    # find the number of observations in the smallest group\n",
    "    nmin = df[label_col_name].value_counts().min()\n",
    "    return (df\n",
    "            # split the dataframe per group\n",
    "            .groupby(label_col_name)\n",
    "            # sample nmin observations from each group\n",
    "            .apply(lambda x: x.sample(nmin))<§§\n",
    "            # recombine the dataframes\n",
    "            .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "df = downsample(df, 'party_label')\n",
    "#X = df['quotation_cleanD'].values\n",
    "#print(len(X))\n",
    "#y = df['party_label'].values\n",
    "#y = np.array([0 if label=='R' else 1 for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7530c151-1b82-409f-94f0-43c77cdee343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6343692\n"
     ]
    }
   ],
   "source": [
    "X = df['quotation_clean'].values\n",
    "print(len(X))\n",
    "y = df['party_label'].values\n",
    "y = np.array([0 if label=='R' else 1 for label in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c516738b-8d0e-43f4-b594-4a9ec3c0e3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['peopl', 'go', 'state', 'need', 'get', 'think', 'make', 'work',\n",
       "       'presid', 'time', 'one', 'want', 'would', 'know', 'like', 'us',\n",
       "       'take', 'right', 'american', 'health', 'come', 'year', 'thing',\n",
       "       'way', 'new', 'trump', 'help', 'say', 'see', 'continu', 'look',\n",
       "       'communiti', 'day', 'support', 'public', 'countri', 'everi',\n",
       "       'realli', 'care', 'back', 'protect', 'mani', 'hous', 'put',\n",
       "       'nation', 'senat', 'well', 'good', 'could', 'lot', 'import',\n",
       "       'famili', 'govern', 'busi', 'use', 'sure', 'tri', 'live',\n",
       "       'democrat', 'vote', 'provid', 'even', 'issu', 'first', 'today',\n",
       "       'got', 'also', 'believ', 'much', 'elect', 'keep', 'home', 'call',\n",
       "       'bill', 'respons', 'place', 'abl', 'polit', 'job', 'person',\n",
       "       'chang', 'done', 'make sure', 'decis', 'togeth', 'case', 'world',\n",
       "       'someth', 'unit', 'happen', 'last', 'week', 'law', 'said', 'hope',\n",
       "       'covid', 'coronavirus', 'test', 'talk', 'made', 'citi', 'long',\n",
       "       'best', 'must', 'system', 'two', 'number', 'great', 'better',\n",
       "       'let', 'virus', 'crisi', 'give', 'ensur', 'action', 'still',\n",
       "       'servic', 'may', 'ask', 'end', 'feder', 'never', 'clear', 'part',\n",
       "       'money', 'offic', 'move', 'forward', 'school', 'economi', 'differ',\n",
       "       'fund', 'process', 'spread', 'administr', 'start', 'unit state',\n",
       "       'step', 'fight', 'tell', 'america', 'opportun', 'life', 'around',\n",
       "       'allow', 'plan', 'next', 'bring', 'mean', 'republican', 'includ',\n",
       "       'across', 'order', 'act', 'parti', 'power', 'concern', 'stay',\n",
       "       'econom', 'inform', 'local', 'close', 'polici', 'risk', 'point',\n",
       "       'secur', 'million', 'worker', 'possibl', 'posit', 'hard', 'fact',\n",
       "       'everyon', 'congress', 'understand', 'direct', 'safe', 'run',\n",
       "       'feel', 'member', 'question', 'kind', 'impeach', 'impact',\n",
       "       'without', 'deal', 'legisl', 'face', 'campaign', 'pass', 'open',\n",
       "       'effort', 'creat', 'problem', 'find', 'critic', 'governor',\n",
       "       'safeti', 'social', 'show', 'everyth', 'commit', 'futur', 'donald',\n",
       "       'anoth', 'increas', 'emerg', 'win', 'leader', 'stand', 'pay',\n",
       "       'build', 'whether', 'interest', 'health care', 'strong',\n",
       "       'donald trump', 'term', 'serious', 'alway', 'alreadi', 'month',\n",
       "       'hospit', 'actual', 'presid trump', 'war', 'hear', 'biden',\n",
       "       'reason', 'public health', 'address', 'student', 'stop', 'medic',\n",
       "       'big', 'women', 'real', 'littl', 'resourc', 'challeng', 'access',\n",
       "       'program', 'pandem', 'individu', 'high', 'depart', 'counti',\n",
       "       'certain', 'american peopl', 'serv', 'tax', 'hand', 'cannot',\n",
       "       'industri', 'line', 'educ', 'constitut', 'small', 'meet', 'major',\n",
       "       'ever', 'level', 'prepar', 'situat', 'forc', 'focus', 'matter',\n",
       "       'compani', 'addit', 'result', 'lead', 'effect', 'love', 'anyth',\n",
       "       'trial', 'base', 'requir', 'expect', 'remain', 'seen', 'china',\n",
       "       'iran', 'histori', 'voter', 'three', 'attack', 'play', 'wit',\n",
       "       'turn', 'offici', 'someon', 'necessari', 'cost', 'respond',\n",
       "       'becom', 'enough', 'report', 'away', 'york', 'thought'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_w = pd.read_csv(\"top_20_overall.csv\")\n",
    "common_w['phrase'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6c6aeb9-398e-4f57-a604-340cf321a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x):\n",
    "    arr = x.split(\" \")\n",
    "    return any(x in common_w['phrase'].values for x in arr)\n",
    "\n",
    "\n",
    "mask = df['quotation_clean'].apply(lambda x: process(x))\n",
    "X= df['quotation_clean'][mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "637b398e-1a86-44b6-a54c-8fd35f66a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>id</th>\n",
       "      <th>party_label</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>quotation_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-06-040043</td>\n",
       "      <td>I've already started working with my colleague...</td>\n",
       "      <td>eliot engel</td>\n",
       "      <td>2020-01-06 01:53:44</td>\n",
       "      <td>1</td>\n",
       "      <td>Q1329618</td>\n",
       "      <td>D</td>\n",
       "      <td>E000179</td>\n",
       "      <td>already started working colleagues new legisla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-02-039720</td>\n",
       "      <td>I have known Islam on three continents before ...</td>\n",
       "      <td>barack obama</td>\n",
       "      <td>2018-11-02 04:06:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Q76</td>\n",
       "      <td>D</td>\n",
       "      <td>O000167</td>\n",
       "      <td>known islam three continents coming region fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-06-101815</td>\n",
       "      <td>tax breaks for the wealthiest one percent</td>\n",
       "      <td>nancy pelosi</td>\n",
       "      <td>2018-02-06 09:00:06</td>\n",
       "      <td>1</td>\n",
       "      <td>Q170581</td>\n",
       "      <td>D</td>\n",
       "      <td>P000197</td>\n",
       "      <td>tax breaks wealthiest one percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-22-083854</td>\n",
       "      <td>When you go on a mission, you don't ask, `Who ...</td>\n",
       "      <td>mikie sherrill</td>\n",
       "      <td>2018-04-22 11:27:02</td>\n",
       "      <td>1</td>\n",
       "      <td>Q47087146</td>\n",
       "      <td>D</td>\n",
       "      <td>S001207</td>\n",
       "      <td>go mission ask democrat republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-17-000547</td>\n",
       "      <td>[ T ] oo often, consumers don't have the basic...</td>\n",
       "      <td>eric schneiderman</td>\n",
       "      <td>2018-04-17 15:20:39</td>\n",
       "      <td>5</td>\n",
       "      <td>Q5387459</td>\n",
       "      <td>D</td>\n",
       "      <td>None</td>\n",
       "      <td>oo often consumers basic facts need assess fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343687</th>\n",
       "      <td>2017-12-22-070993</td>\n",
       "      <td>significant long-term effort to heal the polit...</td>\n",
       "      <td>heather nauert</td>\n",
       "      <td>2017-12-22 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Q5694014</td>\n",
       "      <td>R</td>\n",
       "      <td>None</td>\n",
       "      <td>significant long term effort heal political di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343688</th>\n",
       "      <td>2016-02-16-051709</td>\n",
       "      <td>I'm 44, I feel 46. I messed up the joke -- I'l...</td>\n",
       "      <td>marco rubio</td>\n",
       "      <td>2016-02-16 18:52:50</td>\n",
       "      <td>2</td>\n",
       "      <td>Q324546</td>\n",
       "      <td>R</td>\n",
       "      <td>R000595</td>\n",
       "      <td>feel messed joke may feel happens repeat somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343689</th>\n",
       "      <td>2017-12-20-011661</td>\n",
       "      <td>become clear that Congress will only be able t...</td>\n",
       "      <td>susan collins</td>\n",
       "      <td>2017-12-20 18:30:32</td>\n",
       "      <td>2</td>\n",
       "      <td>Q22279</td>\n",
       "      <td>R</td>\n",
       "      <td>C001035</td>\n",
       "      <td>become clear congress able pass another short ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343690</th>\n",
       "      <td>2018-05-07-106756</td>\n",
       "      <td>Those people (against brothels) don't understa...</td>\n",
       "      <td>dennis hof</td>\n",
       "      <td>2018-05-07 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Q5258518</td>\n",
       "      <td>R</td>\n",
       "      <td>None</td>\n",
       "      <td>people brothels understand nevada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343691</th>\n",
       "      <td>2018-07-17-046386</td>\n",
       "      <td>It is important that President Trump met with ...</td>\n",
       "      <td>lee zeldin</td>\n",
       "      <td>2018-07-17 22:39:16</td>\n",
       "      <td>1</td>\n",
       "      <td>Q16221257</td>\n",
       "      <td>R</td>\n",
       "      <td>Z000017</td>\n",
       "      <td>important president trump met putin america ad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6343692 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   quoteID                                          quotation  \\\n",
       "0        2020-01-06-040043  I've already started working with my colleague...   \n",
       "1        2018-11-02-039720  I have known Islam on three continents before ...   \n",
       "2        2018-02-06-101815          tax breaks for the wealthiest one percent   \n",
       "3        2018-04-22-083854  When you go on a mission, you don't ask, `Who ...   \n",
       "4        2018-04-17-000547  [ T ] oo often, consumers don't have the basic...   \n",
       "...                    ...                                                ...   \n",
       "6343687  2017-12-22-070993  significant long-term effort to heal the polit...   \n",
       "6343688  2016-02-16-051709  I'm 44, I feel 46. I messed up the joke -- I'l...   \n",
       "6343689  2017-12-20-011661  become clear that Congress will only be able t...   \n",
       "6343690  2018-05-07-106756  Those people (against brothels) don't understa...   \n",
       "6343691  2018-07-17-046386  It is important that President Trump met with ...   \n",
       "\n",
       "                   speaker                date  numOccurrences         id  \\\n",
       "0              eliot engel 2020-01-06 01:53:44               1   Q1329618   \n",
       "1             barack obama 2018-11-02 04:06:00               1        Q76   \n",
       "2             nancy pelosi 2018-02-06 09:00:06               1    Q170581   \n",
       "3           mikie sherrill 2018-04-22 11:27:02               1  Q47087146   \n",
       "4        eric schneiderman 2018-04-17 15:20:39               5   Q5387459   \n",
       "...                    ...                 ...             ...        ...   \n",
       "6343687     heather nauert 2017-12-22 00:00:00               3   Q5694014   \n",
       "6343688        marco rubio 2016-02-16 18:52:50               2    Q324546   \n",
       "6343689      susan collins 2017-12-20 18:30:32               2     Q22279   \n",
       "6343690         dennis hof 2018-05-07 02:00:00               1   Q5258518   \n",
       "6343691         lee zeldin 2018-07-17 22:39:16               1  Q16221257   \n",
       "\n",
       "        party_label US_congress_bio_ID  \\\n",
       "0                 D            E000179   \n",
       "1                 D            O000167   \n",
       "2                 D            P000197   \n",
       "3                 D            S001207   \n",
       "4                 D               None   \n",
       "...             ...                ...   \n",
       "6343687           R               None   \n",
       "6343688           R            R000595   \n",
       "6343689           R            C001035   \n",
       "6343690           R               None   \n",
       "6343691           R            Z000017   \n",
       "\n",
       "                                           quotation_clean  \n",
       "0        already started working colleagues new legisla...  \n",
       "1        known islam three continents coming region fir...  \n",
       "2                        tax breaks wealthiest one percent  \n",
       "3                       go mission ask democrat republican  \n",
       "4        oo often consumers basic facts need assess fai...  \n",
       "...                                                    ...  \n",
       "6343687  significant long term effort heal political di...  \n",
       "6343688  feel messed joke may feel happens repeat somet...  \n",
       "6343689  become clear congress able pass another short ...  \n",
       "6343690                  people brothels understand nevada  \n",
       "6343691  important president trump met putin america ad...  \n",
       "\n",
       "[6343692 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a4dc4-19c5-4422-8846-6efc1bd4def0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ced57-6977-47fd-8100-dcc7506c26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=4)\n",
    "#This works for individual job description, but how to save the model? \n",
    "tdidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "fitted_vectorizer = tdidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33121418-7632-426d-bc93-2c2160a49d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "#nb_tfidf.fit(tfidf_vectorizer_vectors, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cf140-a247-4ef2-8160-67363f2eb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(nb_tfidf, tfidf_vectorizer_vectors, y_train, cv=5)\n",
    "\n",
    "print(f'Avg: {scores.mean():.3f}\\tStd: {scores.std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e2c52-feb8-4410-a69f-2d53b03766c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict y value for test dataset\n",
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(tfidf_vectorizer_vectors, y_train)\n",
    "y_predict = nb_tfidf.predict(fitted_vectorizer.transform(X_test))\n",
    "y_prob = nb_tfidf.predict_proba(fitted_vectorizer.transform(X_test))[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220819d9-6fe8-4561-b4c5-c0a4da96aca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
